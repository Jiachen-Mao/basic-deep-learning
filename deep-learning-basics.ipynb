{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.nn.relu(tf.matmul(x, w1) + bias1)\n",
    "b = tf.nn.relu(tf.matmul(a, w2) + bias2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss defined in TensorFlow\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "\n",
    "v1 = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "v2 = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print((v1*v2).eval())\n",
    "print(tf.matmul(v1, v2).eval())\n",
    "\n",
    "v = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "print(tf.reduce_mean(v).eval())\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(label = y_, logits = y)\n",
    "\n",
    "mse = tf.reduce_mean(tf.square(y_ - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss defined by users\n",
    "\n",
    "v1 = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "v2 = tf.constant([4.0, 3.0, 2.0, 1.0])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "print(tf.greater(v1, v2).eval())\n",
    "\n",
    "print(tf.where(tf.greater(v1, v2), v1, v2).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "...\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    for i in range(STEPS):\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {x: current_X, y: current_Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential decay\n",
    "\n",
    "decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "\n",
    "\n",
    "# tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase = True/False)\n",
    "\n",
    "global_step = tf.Variable(0)\n",
    "...\n",
    "learning_rate = tf.train.exponential_decay(0.1, global_step, 100, 0.96, staircase = True)\n",
    "...\n",
    "learning_rate = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random_normal([2, 1], stddev = 1, seed = 1))\n",
    "y = tf.matmul(x, w)\n",
    "...\n",
    "loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda_)(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(shape, lambda_):\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype = tf.float32)\n",
    "    # Use add_to_collection to add the L2 regularizer wrt the variable\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(lambda_)(var))\n",
    "    return var\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape = (None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape = (None, 1))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "layer_dimension = [2, 10, 10, 10, 1]\n",
    "\n",
    "n_layers = len(layer_dimension)\n",
    "\n",
    "current_layer = x\n",
    "\n",
    "in_dimension = layer_dimension[0]\n",
    "\n",
    "for i in range(1, n_layers):\n",
    "    out_dimension = layer_dimension[i]\n",
    "    \n",
    "    weight = get_weight([in_dimension, out_dimension], 0.001)\n",
    "    bias = tf.Variable(tf.constant(0.1, shape = [out_dimension]))\n",
    "    \n",
    "    cur_layer = tf.nn.relu(tf.matmul(cur_layer, weight) + bias)\n",
    "    \n",
    "    in_dimension = layer_dimension[i]\n",
    "    \n",
    "mse_loss = tf.reduce_mean(tf.square(y_ - cur_layer))\n",
    "\n",
    "tf.add_collection('losses', mse_loss)\n",
    "\n",
    "loss = tf.add_n(tf.get_collection('losses'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tf.Variable(0, dtype = tf.float32)\n",
    "step = tf.Variable(0, trainable = False)\n",
    "\n",
    "ema = tf.train.ExponentialMovingAverage(0.99, step)\n",
    "maintain_averages_op = ema.apply([v1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    sess.run(tf.assign(v1, 5))\n",
    "    sess.run(maintain_averages_op)\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    sess.run(tf.assign(step, 10000))\n",
    "    sess.run(tf.assign(v1, 10))\n",
    "    sess.run(maintain_averages_op)\n",
    "    print(sess.run([v1, ema.average(v1)]))\n",
    "    \n",
    "    sess.run(maintain_averages_op)\n",
    "    print(sess.run([v1, ema.average(v1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
